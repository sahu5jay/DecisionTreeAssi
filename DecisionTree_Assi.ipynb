{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "127f2787-3981-4999-b0fa-6b77a3d7f033",
   "metadata": {},
   "source": [
    "Que-1  What is a Decision Tree, and how does it work in the context of classification? \n",
    "\n",
    "Ans-1 A Decision Tree is a supervised machine learning algorithm that classifies data by repeatedly splitting it based on feature values, forming a tree-like structure where each leaf node represents a class decision.\n",
    "A decision tree is a supervised learning algorithm that classifies data using a series of rule-based splits, and it is widely used in marketing for customer targeting, logistics for delay prediction, and finance for credit risk assessment due to its interpretability.\n",
    "Finance Use Case – Loan Default Prediction\n",
    "Logistics Use Case – Delivery Delay Classification\n",
    "Marketing Use Case – Customer Conversion Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94faacc2-352d-47ff-b0e3-35d94cc984b8",
   "metadata": {},
   "source": [
    "Que-2 Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree? \n",
    "Ans-2 In a Decision Tree, Gini Impurity and Entropy are impurity measures used to decide how to split the data at each node.\n",
    "Gini Impurity- Gini Impurity measures the probability of incorrect classification if a data point is randomly labeled according to the class distribution at a node.     \n",
    "            Gini=1−∑pi2\n",
    "Entropy- Entropy measures the uncertainty or randomness in the data.   Entropy=−∑pi*log2(pi)\n",
    "\n",
    "Information Gain (for Entropy)\n",
    "IG=Entropy(parent)−Weighted Entropy(children)\n",
    "IG=Entropy(parent)−Weighted Entropy(children)\n",
    "Gini Reduction\n",
    "Gini Gain= Gini(parent)−Weighted Gini(children)\n",
    "Gini Gain=Gini(parent)−Weighted Gini(children)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1c646a-9b59-4bcc-9576-c1aa8cb6177b",
   "metadata": {},
   "source": [
    "Que-3  What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
    "Ans-3 Pre-Pruning (Early Stopping) - Pre-pruning stops the tree from growing further once certain conditions are met, before it becomes too complex.\n",
    "Post-Pruning (Pruning After Full Growth) - Post-pruning allows the tree to grow fully first, and then removes branches that do not improve performance on validation data.\n",
    "Pre-pruning controls tree growth early to reduce complexity and training time, while post-pruning trims a fully grown tree to improve generalization by removing overfitting branches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65632887-aacf-44f1-8736-4e54ea2ecf1e",
   "metadata": {},
   "source": [
    "Que-4  What is Information Gain in Decision Trees, and why is it important for choosing the best split? \n",
    "Ans-4 Information Gain (IG) is a metric used in Decision Trees (especially with Entropy) to measure how much uncertainty is reduced after splitting the data on a particular feature.\n",
    "Information Gain measures the reduction in entropy achieved by a split, and it is important because decision trees choose the feature with the highest information gain to create the most informative and pure child nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950597e8-face-418a-8fa5-a241eac6076b",
   "metadata": {},
   "source": [
    "Que-5 What are some common real-world applications of Decision Trees, and what are their main advantages and limitations? \n",
    "Ans-5 A Decision Tree is a supervised machine learning algorithm widely used in business and analytics because its logic is rule-based and easy to interpret.\n",
    "A company predicts whether a customer will respond to a discount offer based on age, income, and past purchases.\n",
    "Banks classify applicants as low risk or high risk using credit score, income, and existing liabilities.\n",
    "Advantages of Decision Trees\n",
    "\n",
    "Easy to Interpret & Explain\n",
    "Handles Non-Linear Relationships\n",
    "Minimal Data Preprocessing\n",
    "Works with Numerical & Categorical Data\n",
    "\n",
    "Limitations of Decision Trees\n",
    "Overfitting\n",
    "Solution: pruning, max depth, Random Forest.\n",
    "Unstable- Small data changes can produce very different trees.\n",
    "Lower Accuracy Compared to Ensembles- Single trees are weaker than Random Forest or XGBoost.\n",
    "Bias Toward Dominant Features- Can favor features with more split points.\n",
    "Decision trees are widely used in marketing, finance, logistics, and healthcare for classification and decision-making due to their interpretability, but they suffer from overfitting and instability, which is why ensemble methods are often preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "178c0b47-b423-48d2-9ac8-e556b461c5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, 1, 2, 0, 2, 2, 2, 1, 0, 1, 0, 2, 0, 0, 2, 1, 0, 2, 1, 2,\n",
       "       2, 1, 2, 1, 0, 1, 0, 1, 0, 1, 1, 2, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Que-6 Write a Python program to: \n",
    "# ● Load the Iris Dataset \n",
    "# ● Train a Decision Tree Classifier using the Gini criterion\n",
    "# ● Print the model’s accuracy and feature importances  \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state = 44)\n",
    "model = DecisionTreeClassifier(criterion = 'gini', random_state = 42)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04e5aae6-b5ac-4c44-8416-f931718a3899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd07b8dc-adce-4105-8ff5-e2b8998eba2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm): 0.025894921470142718\n",
      "sepal width (cm): 0.0\n",
      "petal length (cm): 0.9293248785154677\n",
      "petal width (cm): 0.044780200014389517\n"
     ]
    }
   ],
   "source": [
    "for feature, importance in zip(iris.feature_names, model.feature_importances_):\n",
    "    print(f\"{feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85ee6750-6902-4e88-90c9-ddc8c17091eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model without pruning :  0.8947368421052632\n",
      "Accuracy of the model with pruning :  0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "# Que-7 Write a Python program to: \n",
    "# ● Load the Iris Dataset \n",
    "# ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree. \n",
    "\n",
    "model_full_len = DecisionTreeClassifier(random_state = 44)\n",
    "model_full_len.fit(x_train, y_train)\n",
    "y_pred_fulllen = model_full_len.predict(x_test)\n",
    "\n",
    "model_with_prun = DecisionTreeClassifier(max_depth = 3, random_state = 44)\n",
    "model_with_prun.fit(x_train, y_train)\n",
    "y_pred_wih_prun = model_with_prun.predict(x_test)\n",
    "print(\"Accuracy of the model without pruning : \", accuracy_score(y_test, y_pred_fulllen))\n",
    "print(\"Accuracy of the model with pruning : \", accuracy_score(y_test, y_pred_wih_prun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15a69800-9f9f-4e93-9a3d-8da50cc8e439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 16.68842519685039\n"
     ]
    }
   ],
   "source": [
    "# Que-8 Write a Python program to: \n",
    "# ● Load the Boston Housing Dataset \n",
    "# ● Train a Decision Tree Regressor \n",
    "# ● Print the Mean Squared Error (MSE) and feature importances \n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "df = fetch_openml(name=\"boston\", version=1, as_frame=False)\n",
    "x = df.data \n",
    "y = df.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n",
    "model = DecisionTreeRegressor(random_state = 42)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "print(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d48d007f-c92e-454c-81a1-3c89dbe0a58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
      "Model Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "param_grid = {\"max_depth\": [2, 3, 4, 5, None],\"min_samples_split\": [2, 5, 10]}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Model Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d3cb25-1ae3-4311-abec-4c8718900566",
   "metadata": {},
   "source": [
    "Que-10 Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain\n",
    "disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to: \n",
    "● Handle the missing values \n",
    "● Encode the categorical features \n",
    "● Train a Decision Tree model \n",
    "● Tune its hyperparameters \n",
    "● Evaluate its performance And describe what business value this model could provide in the real-world setting. \n",
    "\n",
    "Ans-10 \n",
    "Handle Missing Values\n",
    "\n",
    "Numerical features → Impute with:\n",
    "    Mean (if data is normally distributed)\n",
    "    Median (if data is skewed or has outliers)\n",
    "\n",
    "Categorical features → Impute with:\n",
    "    Mode (most frequent value)\n",
    "    Or a category like \"Unknown\"\n",
    "    ex- Missing cholesterol values might be replaced with the median to avoid skewing due to extreme cases.\n",
    "\n",
    "Encode Categorical Features\n",
    "    Use Label Encoding for binary categories (Yes/No)\n",
    "    Use One-Hot Encoding for multi-category features (e.g., blood group, region)\n",
    "    Smoking status → {Non-smoker, Former, Current} → One-Hot Encoding\n",
    "\n",
    "Train a Decision Tree Model\n",
    "Split data into training and testing sets\n",
    "Train a DecisionTreeClassifier\n",
    "Start with default parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
